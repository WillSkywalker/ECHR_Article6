# -*- coding: utf-8 -*-
"""pipeline.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/0B0JBOYNckNX-a05WNlNVQ1hsZ3YtTndEUnNhQkRCZE1WUUEw
"""

# import everything
 #!/usr/bin/python
 # -*- coding: utf-8 -*-x
import glob, os #packages to work with folders
import pandas as pd #package to work with CSV tables
import re #regular expressions
from nltk import word_tokenize,sent_tokenize,bigrams 

!pip install langdetect
!pip install textract

from langdetect import detect
import textract

#check if the text is in english
def make_eng_txt(file):
    text = textract.process(file, encoding='utf-8', extension='docx').decode("utf-8", "ignore") # open docx
    if detect(text) != 'en': #check lang
        return
    text = text.split('\n')
    lines = [i for i in text if i != '' and i != '\t*'] #remove empty lines
    return lines

def extract_app_num(lines, f):
    app_nums = []
    lines_top = lines
    for num in range(len(lines)):
        lines[num] = re.sub('v', 'V', lines[num])
        lines[num] = re.sub('\t', '', lines[num])
        lines[num] = re.sub('^\d+$', '', lines[num])
    lines = [line for line in lines if line.isupper() == False] #remove titles and footings
    lines = [line for line in lines if line != ''] #remove empty lines
    if len(lines) > 5: #only first five lines to avoid extracting cited case numbers
        lines_top = lines[:5]
    for line in lines_top:
        tuples = re.findall('([0-9]{2,8}\/[0-9]{2})([^0-9/]|$)', line) #extract application numbers
        srch = [t[0] for t in tuples]
        if len(srch)>0:
            app_nums.extend(srch)
        else:
            lines_top = lines[:50]
            for line in lines_top:
                tuples = re.findall('([0-9]{2,8}\/[0-9]{2})([^0-9/]|$)', line) #extract application numbers
                srch = [t[0] for t in tuples]
                if len(srch)>0:
                    app_nums.extend(srch)
                    break
            
    other_files = []
    if len(app_nums)==0:
        print('FAILED', f, lines_top)
        extra_docs.append(f)
        return
    return tuple([tuple(set(app_nums)), f])

# communicated cases extract text
#load files
files_comm = glob.glob('./data/*') #communicated cases
d_comm_texts = {}
count = 0
for f in files_comm:
    count+=1
    #print(count, f)
    try:
        case_text = make_eng_txt(f)
        #application number
        if case_text != None:
            app_n = extract_app_num(make_eng_txt(f), f)
            if app_n != None:
                d_comm_texts[app_n] = case_text
    except:
        print('ERROR', f)
        pass

#judgements extract text

judgements = glob.glob('../crystal_ball_mix/data/*/*/*.docx')
case_texts_judgements = []
count = 0
d_judgements_texts = {}
for f in judgements:
    count+=1
    #print(count, f)
    try:
        case_text_judgements = make_eng_txt(f)
        if case_text_judgements != None:
            app_n = extract_app_num(make_eng_txt(f), f)
            if app_n != None:
                d_judgements_texts[app_n] = case_text_judgements
    except:
        print('ERROR', f)
        pass

#other extract text

files_adm = glob.glob('./admissibility_commitee/*')  + glob.glob('./admissibility/*') 
case_texts_admissibility = []
count = 0
d_admissibility_texts = {}
for f in files_adm:
    count+=1
    #print(count, f)
    try:
        case_text_admissibility = make_eng_txt(f)
    #application number
        if case_text_admissibility != None:
            app_n = extract_app_num(make_eng_txt(f), f)
            if app_n != None:
                d_admissibility_texts[app_n] = case_text_admissibility
    except:
        print('ERROR', f)
        pass

#judgements extract application numbers
all_apps = []
for item in list(d_judgements_texts.keys()):
    all_apps.extend(list(item[0]))
all_apps_judgements = list(set(all_apps))

#communication reports extract application numbers
all_apps = []
for item in list(d_comm_texts.keys()):
    all_apps.extend(list(item[0]))
all_apps_comms = list(set(all_apps))

#addmissibility extract application numbers
all_apps = []
for item in list(d_admissibility_texts.keys()):
    all_apps.extend(list(item[0]))
all_apps_admissibility = list(set(all_apps))

len(all_apps_judgements), len(all_apps_comms), len(all_apps_admissibility)

b = set(all_apps_judgements).intersection(set(all_apps_comms))
print(len(b))

b = [i for i in all_apps_comms if i in all_apps_judgements]
len(list(set(b)))

#c = list(b.difference(set(all_apps_comms)))+ list(b.difference(set(all_apps_admissibility)))
c = [i for i in all_apps_comms if i not in b] + [i for i in all_apps_admissibility if i not in b] + [i for i in all_apps_judgements if i not in b]
print(len(list(set(c))))
print(c[0])
#c = list(set(c))

#judgements
#create table #app_num #violations per article 0/1/2/3
#0 - no violation
#1 - violation
#3 - both 
#2 - not ruled on
articles = [2,3,4,5,6,7,8,9,10,11,12,13,14,18]
d_judgements = {}
for app in all_apps_judgements:
    tbl = [2]*14
    for item in list(d_judgements_texts.keys()):
        if app in item[0]:
            for art in articles:
        #print(art)
                if re.search('Article'+str(art), item[1]) != None:
                    if re.search('/violation/', item[1]) != None:
                        if tbl[articles.index(art)] == 2: #did not happen
                            tbl[articles.index(art)] = 1
                        elif tbl[articles.index(art)] == 0 or tbl[articles.index(art)] == 3:
                            tbl[articles.index(art)] = 3 #both violation and not
                            
                    else:
                        if tbl[articles.index(art)] == 2:
                            tbl[articles.index(art)] = 0
                        elif tbl[articles.index(art)] == 1 or tbl[articles.index(art)] == 3:
                            tbl[articles.index(art)] = 3
    #tbl.append(app)
    d_judgements[app] = tuple(tbl)
#print(d_judgements)

#dataset for training and testing
dataset = {}
for app in b:
    for item in d_comm_texts.keys():
        #print(d_comm_texts[item])
        if app in list(item[0]):
            frfr = [i[0] for i in list(d_comm_texts.keys())]
            if item[0] in frfr and item[0][0] in list(d_judgements.keys()):
                dataset[tuple(d_comm_texts[item])] = d_judgements[item[0][0]]

dataset_word2vec = []
for app in c:
    for item in d_comm_texts.keys():
        if app in list(item[0]):
            if d_comm_texts[item] not in dataset_word2vec:
                dataset_word2vec.append(d_comm_texts[item])
    for item in d_admissibility_texts.keys():
        if app in list(item[0]):
            if d_admissibility_texts[item] not in dataset_word2vec:
                dataset_word2vec.append(d_admissibility_texts[item])
    for item in d_judgements_texts.keys():
        if app in list(item[0]):
            if d_judgements_texts[item] not in dataset_word2vec:
                dataset_word2vec.append(d_judgements_texts[item])

print(len(c), len(dataset_word2vec))

def train_per_article(art):
    labels = {}
    for v in list(dataset.keys()):
        if dataset[v][articles.index(art)] == 1 or dataset[v][articles.index(art)] == 3:
            labels[v] = 1
        if dataset[v][articles.index(art)] == 0 or dataset[v][articles.index(art)] == 2:
            labels[v] = 0


    Xtrain = []
    Ytrain = []
    for item in list(labels.keys()):
        Xtrain.extend(tokenize_data_word(list(item)))
        Ytrain.append(labels[item])
    cut = round(len(Xtrain)/100*80)
    Xtest = Xtrain[cut:]
    Ytest = Ytrain[cut:]
    Xtrain = Xtrain[:cut]
    Ytrain = Ytrain[:cut]
    return Xtrain, Ytrain, Xtest, Ytest
    #if v[1] == 3:
        #count_b+=1
#print(articles[num], count_v, count_nv, count_b)

def balance_dataset_per_article(Xtrain, Ytrain):
    Xtrain_new = []
    Ytrain_new = []
    limit = Ytrain.count(0)
    count = 0

    for i in range(len(Ytrain)):
        if count < limit:
            if Ytrain[i] == 0:
                #print('bla')
                count +=1
                Xtrain_new.append(Xtrain[i])
                Ytrain_new.append(Ytrain[i])

    for i in range(len(Ytrain)):
        if Ytrain[i] == 1:
            Xtrain_new.append(Xtrain[i])
            Ytrain_new.append(Ytrain[i])
    Xtrain = Xtrain_new
    Ytrain = Ytrain_new
    return Xtrain, Ytrain

def balance_dataset(Xtrain, Ytrain):
    Xtrain_new = []
    Ytrain_new = []
    limit = Ytrain.count(1)
    count = 0

    for i in range(len(Ytrain)):
        if count < limit:
            if Ytrain[i] == 0:
                #print('bla')
                count +=1
                Xtrain_new.append(Xtrain[i])
                Ytrain_new.append(Ytrain[i])

    for i in range(len(Ytrain)):
        if Ytrain[i] == 1:
            Xtrain_new.append(Xtrain[i])
            Ytrain_new.append(Ytrain[i])
    Xtrain = Xtrain_new
    Ytrain = Ytrain_new
    return Xtrain, Ytrain

articles = [2,3,4,5,6,7,8,9,10,11,12,13,14,18]

for num in range(len(articles)):
    count_v = 0
    count_nv = 0
    count_b = 0
    for v in list(dataset.values()):
        if v[num] == 1:
            count_v+=1
        if v[num] == 0:
            count_nv+=1
        if v[num] == 3:
            count_b+=1
    print(articles[num], count_v, count_nv, count_b)

def tokenize_data_word(lines):
    tokenized_text = []
    
    for num in range(len(lines)):
        lines[num] = re.sub('\n', '', lines[num])
    lines = ' '.join(lines)

    words = word_tokenize(lines)
    words = [i.lower() for i in words]
    tokenized_text.append(words)
     
    return tokenized_text

tokenized_data = []
for i in dataset_word2vec:
    tokenized_data.extend(tokenize_data_word(i))
print('vectors', len(tokenized_data))

#import word2vec stuff
from gensim.models import Word2Vec
from sklearn.decomposition import PCA
from matplotlib import pyplot

from sklearn.manifold import TSNE
# Need the interactive Tools for Matplotlib
# %matplotlib notebook
import numpy as np
import matplotlib.pyplot as plt

model = Word2Vec.load('model_cbow')

import gensim

model = gensim.models.KeyedVectors.load_word2vec_format('./model_cbow_w2v', binary=False)
with open( 'tensorsfp.tsv', 'w+') as tensors:
    with open( 'metadatafp.tsv', 'w+') as metadata:
        for word in model.wv.index2word:
            encoded=word.encode('utf-8', 'replace')
            if not isinstance(encoded, str):
                encoded = encoded.decode("utf-8")
            metadata.write(encoded + '\n')
            vector_row = '\t'.join(map(str, model[word]))
            tensors.write(vector_row + '\n')
        print(len(metadata.readlines()))

#print(model['computer']) 
model.similar_by_word('yudkivska')

def display_closestwords_tsnescatterplot(model, word):
    
    arr = np.empty((0,200), dtype='f')
    word_labels = [word]

    # get close words
    close_words = model.most_similar_cosmul(word, topn=10)
    #close_words = ['albania', 'andorra', 'armenia', 'austria', 'azerbaijan', 'belgium', 'bosnia', 'bulgaria', 'croatia', 'cyprus', 'czech', 'denmark', 'estonia', 'finland', 'france', 'georgia', 'germany', 'greece', 'hungary', 'iceland', 'ireland', 'italy', 'latvia', 'lithuania', 'luxembourg', 'malta', 'monaco', 'netherlands', 'norway', 'poland', 'portugal', 'moldova', 'romania', 'russia', 'russia', 'marino', 'serbia', 'slovak', 'slovenia', 'spain', 'sweden', 'switzerland', 'macedonia', 'kingdom', 'turkey', 'ukraine',  'albania', 'andorra', 'armenia', 'austria', 'azerbaijan', 'belgium', 'bosnia', 'bulgaria', 'croatia', 'cyprus', 'czech', 'denmark', 'estonia', 'finland', 'france', 'germany', 'hungary']
    close_words = list(set(close_words))
    # add the vector for each of the closest words to the array
    arr = np.append(arr, np.array([model[word]]), axis=0)
    for wrd_score in close_words:
        wrd_vector = model[wrd_score[0]]
        word_labels.append(wrd_score[0])
        arr = np.append(arr, np.array([wrd_vector]), axis=0)
        
    # find tsne coords for 2 dimensions
    tsne = TSNE(n_components=2, random_state=0)
    np.set_printoptions(suppress=True)
    Y = tsne.fit_transform(arr)

    x_coords = Y[:, 0]
    y_coords = Y[:, 1]
    # display scatter plot
    plt.scatter(x_coords, y_coords)

    for label, x, y in zip(word_labels, x_coords, y_coords):
        plt.annotate(label, xy=(x, y), xytext=(0, 0), textcoords='offset points')
    plt.xlim(x_coords.min()+0.00005, x_coords.max()+0.00005)
    plt.ylim(y_coords.min()+0.00005, y_coords.max()+0.00005)
    plt.show()

display_closestwords_tsnescatterplot(model, 'russia')

#word2vec/doc2vec training
sentences_all = tokenized_data
model_1 = Word2Vec(sentences_all, min_count=1, workers=14, size = 800)

model_2 = Word2Vec(sentences_all, min_count=1, workers=14, size = 20)

model_3 = Word2Vec(sentences_all, min_count=5, workers=14, size = 200)

# let X be a list of tokenized texts (i.e. list of lists of tokens)
X = tokenized_data
model_sg = gensim.models.Word2Vec(X, size=200, workers=14, sg=1, window=5) #200 best min_count=5 mean skip ng-grams and tfidf iter=5
w2v_sg = dict(zip(model_sg.wv.index2word, model_sg.wv.vectors))

model_cbow = gensim.models.Word2Vec(X, size=200, workers=14, sg=0, window=10) #200 best min_count=5 mean skip ng-grams and tfidf iter=5
w2v_cbow = dict(zip(model_cbow.wv.index2word, model_cbow.wv.vectors))

# let X be a list of tokenized texts (i.e. list of lists of tokens)
#X = tokenized_data
#model = gensim.models.Word2Vec(X, size=200, workers=14, ) #200
w2v_google = dict(zip(model_google.wv.index2word, model_google.wv.vectors))

from collections import defaultdict

class MeanEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.dim = len(word2vec.values())

    def fit(self, X, y):
        return self

    def transform(self, X):#mean
        return np.array([
            np.mean([self.word2vec[w] for w in words if w in self.word2vec]
                    or [np.zeros(self.dim)], axis=0)
            for words in X
        ])

class TfidfEmbeddingVectorizer(object):
    def __init__(self, word2vec):
        self.word2vec = word2vec
        self.word2weight = None
        self.dim = len(word2vec.values())

    def fit(self, X, y):
        tfidf = TfidfVectorizer(analyzer=lambda x: x)
        tfidf.fit(X)
        # if a word was never seen - it must be at least as infrequent
        # as any of the known words - so the default idf is the max of 
        # known idf's
        max_idf = max(tfidf.idf_)
        self.word2weight = defaultdict(
            lambda: max_idf,
            [(w, tfidf.idf_[i]) for w, i in tfidf.vocabulary_.items()])

        return self

    def transform(self, X):
        return np.array([
                np.mean([self.word2vec[w] * self.word2weight[w]
                         for w in words if w in self.word2vec] or
                        [np.zeros(self.dim)], axis=0)
                for words in X
            ])

from sklearn.pipeline import Pipeline
#from sklearn.ensemble import ExtraTreesClassifier
def make_pipeline(w2v):
    etree_w2v = Pipeline([
        ("word2vec vectorizer", MeanEmbeddingVectorizer(w2v)),
        ("extra trees", LinearSVC())])
    etree_w2v_tfidf = Pipeline([
        ("word2vec vectorizer", TfidfEmbeddingVectorizer(w2v)),
        ("extra trees", LinearSVC())])
    for art in [3, 5, 6, 8]:
        print('Article', art)
        Xtrain, Ytrain, Xtest, Ytest = train_per_article(art)
        Ypredict_w2v = cross_val_predict(etree_w2v, Xtrain, Ytrain, cv=10)
        Ypredict_w2v_tfidf = cross_val_predict(etree_w2v_tfidf, Xtrain, Ytrain, cv=10)
        print('Mean vector:\n')
        evaluate(Ytrain, Ypredict_w2v) #0.8934010152284264
        print('TfIdf:\n')
        evaluate(Ytrain, Ypredict_w2v_tfidf) #0.8944162436548223
        print('test')
        etree_w2v.fit(Xtrain, Ytrain)
        etree_w2v_tfidf.fit(Xtrain, Ytrain)
        print('Mean vector:\n')
        Ypredict_w2v_test = etree_w2v.predict(Xtest)
        evaluate(Ytest, Ypredict_w2v_test)
        print('TfIdf:\n')
        Ypredict_w2v_tfidf_test = etree_w2v_tfidf.predict(Xtest)
        evaluate(Ytest, Ypredict_w2v_tfidf_test)

for vec in [w2v_sg, w2v_cbow, w2v_google]:
    print('-----------------------------------------')
    make_pipeline(vec)

#model['torture']
print(model_1.most_similar('torture'))
print(model_2.most_similar('torture'))
print(model_3.most_similar('torture'))

model_sg.save('./model_sg')
model_sg.save('./model_cbow')

import gensim

model_google = gensim.models.KeyedVectors.load_word2vec_format('./GoogleNews-vectors-negative300.bin', binary=True)

#https://towardsdatascience.com/understanding-feature-engineering-part-4-deep-learning-methods-for-text-data-96c44370bbfa

#https://www.kdnuggets.com/2018/04/robust-word2vec-models-gensim.html

#http://www.cse.chalmers.se/~richajo/dit865/files/Word%20embeddings%20in%20Gensim.html

#corpus = [['torture', 'in', 'the', 'court'],['russia', 'is', 'not', 'helping']]
corpus = Xtrain
w2v_feature_array = averaged_word_vectorizer(corpus=corpus, model=model_1, num_features=200)
#pd.DataFrame(w2v_feature_array)

from sklearn.model_selection import cross_val_predict
from sklearn.metrics import accuracy_score
from sklearn.metrics import recall_score, precision_score, f1_score, classification_report
from sklearn.metrics import confusion_matrix
from sklearn.svm import LinearSVC
from sklearn.naive_bayes import GaussianNB, MultinomialNB, BernoulliNB
from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer
from sklearn.pipeline import Pipeline, FeatureUnion
from sklearn.model_selection import GridSearchCV
#svm


#affinity propogation

svm = LinearSVC()
svm.fit(w2v_feature_array, Ytrain)

Ypredict = cross_val_predict(svm, w2v_feature_array, Ytrain, cv=10)

def evaluate(Ytest, Ypredict): #evaluate the model (accuracy, precision, recall, f-score, confusion matrix)
        print('Accuracy:', accuracy_score(Ytest, Ypredict) )
        print('\nClassification report:\n', classification_report(Ytest, Ypredict))
        print('\nConfusion matrix:\n', confusion_matrix(Ytest, Ypredict), '\n\n_______________________\n\n')
        print('F1-score (weighted):', f1_score(Ytest, Ypredict, average='weighted'))
        print('F1-score (macro):', f1_score(Ytest, Ypredict, average='macro'))

evaluate(Ytrain, Ypredict)

evaluate(Ytrain, Ypredict_w2v) #0.8934010152284264

evaluate(Ytrain, Ypredict_w2v_tfidf) #0.8944162436548223

